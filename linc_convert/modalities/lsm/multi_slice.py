"""
Convert a collection of tiff files generated by the LSM pipeline into a Zarr.

Example input files can be found at
https://lincbrain.org/dandiset/000010/draft/files?location=sourcedata%2Fderivatives
"""

# stdlib
import ast
import os
import re
from glob import glob

# externals
import cyclopts
import nibabel as nib
import numpy as np
import zarr
from tifffile import TiffFile

# internals
from linc_convert import utils
from linc_convert.modalities.lsm.cli import lsm
from linc_convert.utils.math import ceildiv
from linc_convert.utils.orientation import center_affine, orientation_to_affine
from linc_convert.utils.zarr.compressor import make_compressor
from linc_convert.utils.zarr.zarr_config import ZarrConfig

multi_slice = cyclopts.App(name="multi_slice", help_format="markdown")
lsm.command(multi_slice)


@multi_slice.default
def convert(
    inp: str,
    *,
    out: str,
    zarr_config: ZarrConfig = None,
    overlap: int = 30,
    max_load: int = 512,
    orientation: str = "coronal",
    center: bool = True,
    thickness: float | None = None,
    voxel_size: list[float] = (1, 1, 1),
    **kwargs
) -> None:
    """
    Convert a collection of tiff files generated by the LSM pipeline into a Zarr.

    Orientation
    -----------
    The anatomical orientation of the slice is given in terms of RAS axes.

    It is a combination of two letters from the set
    `{"L", "R", "A", "P", "I", "S"}`, where

    * the first letter corresponds to the horizontal dimension and
        indicates the anatomical meaning of the _right_ of the jp2 image,
    * the second letter corresponds to the vertical dimension and
        indicates the anatomical meaning of the _bottom_ of the jp2 image.

    We also provide the aliases

    * `"coronal"` == `"LI"`
    * `"axial"` == `"LP"`
    * `"sagittal"` == `"PI"`

    The orientation flag is only useful when converting to nifti-zarr.

    Parameters
    ----------
    inp
        Path to the root directory, which contains a collection of
        subfolders named `*_y{:02d}_z{:02d}*`, each containing a
        collection of files named `*_plane{:03d}_c{:d}.tiff`.
    out
        Path to the output Zarr directory [<INP>.ome.zarr]
    overlap
        Number of pixels between slices that are overlapped
    chunk
        Output chunk size
    compressor : {blosc, zlib, raw}
        Compression method
    compressor_opt
        Compression options
    max_load
        Maximum input chunk size when building pyramid
    nii
        Convert to nifti-zarr. True if path ends in ".nii.zarr".
    orientation
        Orientation of the slice
    center
        Set RAS[0, 0, 0] at FOV center
    voxel_size
        Voxel size along the X, Y and Z dimension, in micron.
    """
    zarr_config = utils.zarr.zarr_config.update(zarr_config, **kwargs)
    chunk: int = zarr_config.chunk[0]
    compressor: str = zarr_config.compressor
    compressor_opt: str = zarr_config.compressor_opt
    nii: bool = zarr_config.nii

    if isinstance(compressor_opt, str):
        compressor_opt = ast.literal_eval(compressor_opt)

    if max_load % 2:
        max_load += 1

    CHUNK_PATTERN = re.compile(
        r"^(?P<prefix>\w*)" r"_y(?P<y>[0-9]+)" r"_z(?P<z>[0-9]+)" r"(?P<suffix>\w*)$"
    )

    all_chunks_filenames = list(sorted(glob(os.path.join(inp, "*_y*_z*"))))
    all_chunks_info = dict(filename=[], prefix=[], suffix=[], z=[], y=[])

    # parse all directory names
    for filename in all_chunks_filenames:
        parsed = CHUNK_PATTERN.fullmatch(
            os.path.splitext(os.path.basename(filename))[0]
        )
        all_chunks_info["filename"].append(filename)
        all_chunks_info["prefix"].append(parsed.group("prefix"))
        all_chunks_info["suffix"].append(parsed.group("suffix"))
        all_chunks_info["z"].append(int(parsed.group("z")))
        all_chunks_info["y"].append(int(parsed.group("y")))

    # default output name
    if not out:
        out = all_chunks_info["prefix"][0] + all_chunks_info["suffix"][0]
        out += ".nii.zarr" if nii else ".ome.zarr"
    nii = nii or out.endswith(".nii.zarr")

    # parse all individual file names
    nchunkz = max(all_chunks_info["z"])
    nchunky = max(all_chunks_info["y"])
    allshapes = [[(0, 0, 0) for _ in range(nchunky)] for _ in range(nchunkz)]

    dtype = None
    for zchunk in range(nchunkz):
        for ychunk in range(nchunky):
            for i in range(len(all_chunks_info["filename"])):
                if (
                    all_chunks_info["z"][i] == zchunk + 1
                    and all_chunks_info["y"][i] == ychunk + 1
                ):
                    break
            filename = all_chunks_info["filename"][i]
            f = TiffFile(filename)
            dtype = f.pages[0].dtype
            allshapes[zchunk][ychunk] = (len(f.pages), *f.pages[0].shape)

    # check that all chunk shapes are compatible
    for zchunk in range(nchunkz):
        if len(set(shape[1] for shape in allshapes[zchunk])) != 1:
            raise ValueError("Incompatible Y shapes")
    for ychunk in range(nchunky):
        if len(set(shape[ychunk][0] for shape in allshapes)) != 1:
            raise ValueError("Incompatible Z shapes")
    if len(set(shape[2] for subshapes in allshapes for shape in subshapes)) != 1:
        raise ValueError("Incompatible X shapes")

    # compute full shape
    fullshape = [0, 0, 0]
    fullshape[0] = sum(shape[0][0] for shape in allshapes)
    fullshape[1] = (
        sum(shape[1] for shape in allshapes[0])
        - (len(set(all_chunks_info["y"])) - 1) * overlap
    )
    fullshape[2] = allshapes[0][0][2]

    # Prepare Zarr group
    omz = zarr.storage.DirectoryStore(out)
    omz = zarr.group(store=omz, overwrite=True)
    print(out)
    # Prepare chunking options
    opt = {
        "chunks": [chunk] * 3,
        "dimension_separator": r"/",
        "order": "F",
        "dtype": np.dtype(dtype).str,
        "fill_value": None,
        "compressor": make_compressor(compressor, **compressor_opt),
    }

    # write first level
    omz.create_dataset("0", shape=fullshape, **opt)
    array = omz["0"]
    print("Write level 0 with shape", fullshape)

    for i, filename in enumerate(all_chunks_info["filename"]):
        chunkz = all_chunks_info["z"][i] - 1
        chunky = all_chunks_info["y"][i] - 1
        f = TiffFile(filename)

        pages = f.pages
        dat = f.asarray()
        if len(set(all_chunks_info["y"])) != 1 and overlap!=0:
            if chunky != 0:
                dat = dat[:, overlap // 2 :, :]
            if chunky != max(all_chunks_info["y"])-1:
                dat = dat[:, :-overlap // 2 - (overlap - 2 * (overlap // 2)), :]

        zstart = sum(shape[0][0] for shape in allshapes[:chunkz])
        ystart = sum(shape[1] - overlap for shape in allshapes[chunkz][:chunky])
        if chunky != 0:
            ystart += overlap // 2
        print(
            f"Write plane "
            f"( {zstart} :{zstart+len(pages)}, {ystart}:{ystart + dat.shape[1]})",
            end="\r",
        )
        slicer = (
            slice(zstart, zstart + len(pages)),
            slice(ystart, ystart + dat.shape[1]),
            slice(None),
        )
        array[slicer] = dat
    print("")

    # build pyramid using median windows
    level = 0
    while any(x > 1 for x in omz[str(level)].shape[-3:]):
        prev_array = omz[str(level)]
        prev_shape = prev_array.shape[-3:]
        level += 1

        new_shape = list(map(lambda x: max(1, x // 2), prev_shape))
        if all(x < chunk for x in new_shape):
            break
        print("Compute level", level, "with shape", new_shape)
        omz.create_dataset(str(level), shape=new_shape, **opt)
        new_array = omz[str(level)]

        nz, ny, nx = prev_array.shape[-3:]
        ncz = ceildiv(nz, max_load)
        ncy = ceildiv(ny, max_load)
        ncx = ceildiv(nx, max_load)

        for cz in range(ncz):
            for cy in range(ncy):
                for cx in range(ncx):
                    print(f"chunk ({cz}, {cy}, {cx}) / ({ncz}, {ncy}, {ncx})", end="\r")

                    dat = prev_array[
                        ...,
                        cz * max_load : (cz + 1) * max_load,
                        cy * max_load : (cy + 1) * max_load,
                        cx * max_load : (cx + 1) * max_load,
                    ]
                    crop = [0 if x == 1 else x % 2 for x in dat.shape[-3:]]
                    slicer = [slice(-1) if x else slice(None) for x in crop]
                    dat = dat[(Ellipsis, *slicer)]
                    pz, py, px = dat.shape[-3:]

                    dat = dat.reshape(
                        [
                            max(pz // 2, 1),
                            min(pz, 2),
                            max(py // 2, 1),
                            min(py, 2),
                            max(px // 2, 1),
                            min(px, 2),
                        ]
                    )
                    dat = dat.transpose([0, 2, 4, 1, 3, 5])
                    dat = dat.reshape(
                        [
                            max(pz // 2, 1),
                            max(py // 2, 1),
                            max(px // 2, 1),
                            -1,
                        ]
                    )
                    dat = np.median(dat, -1)

                    new_array[
                        ...,
                        cz * max_load // 2 : (cz + 1) * max_load // 2,
                        cy * max_load // 2 : (cy + 1) * max_load // 2,
                        cx * max_load // 2 : (cx + 1) * max_load // 2,
                    ] = dat

    print("")
    nblevel = level

    # Write OME-Zarr multiscale metadata
    print("Write metadata")
    multiscales = [
        {
            "version": "0.4",
            "axes": [
                {"name": "z", "type": "space", "unit": "micrometer"},
                {"name": "y", "type": "space", "unit": "micrometer"},
                {"name": "x", "type": "space", "unit": "micrometer"},
            ],
            "datasets": [],
            "type": "median window 2x2x2",
            "name": "",
        }
    ]

    voxel_size = list(map(float, reversed(voxel_size)))
    factor = [1] * 3
    for n in range(nblevel):
        shape = omz[str(n)].shape[-3:]
        multiscales[0]["datasets"].append({})
        level = multiscales[0]["datasets"][-1]
        level["path"] = str(n)

        # We made sure that the downsampling level is exactly 2
        # However, once a dimension has size 1, we stop downsampling.
        if n > 0:
            shape_prev = omz[str(n - 1)].shape[-3:]
            if shape_prev[0] != shape[0]:
                factor[0] *= 2
            if shape_prev[1] != shape[1]:
                factor[1] *= 2
            if shape_prev[2] != shape[2]:
                factor[2] *= 2

        level["coordinateTransformations"] = [
            {
                "type": "scale",
                "scale": [
                    factor[0] * voxel_size[0],
                    factor[1] * voxel_size[1],
                    factor[2] * voxel_size[2],
                ],
            },
            {
                "type": "translation",
                "translation": [
                    (factor[0] - 1) * voxel_size[0] * 0.5,
                    (factor[1] - 1) * voxel_size[1] * 0.5,
                    (factor[2] - 1) * voxel_size[2] * 0.5,
                ],
            },
        ]
    multiscales[0]["coordinateTransformations"] = [
        {"scale": [1.0] * 3, "type": "scale"}
    ]
    omz.attrs["multiscales"] = multiscales

    if not nii:
        print("done.")
        return

    # Write NIfTI-Zarr header
    # NOTE: we use nifti2 because dimensions typically do not fit in a short
    # TODO: we do not write the json zattrs, but it should be added in
    #       once the nifti-zarr package is released
    shape = list(reversed(omz["0"].shape))
    shape = shape[:3] + [1] + shape[3:]  # insert time dimension
    affine = orientation_to_affine(orientation, *voxel_size)
    if center:
        affine = center_affine(affine, shape[:3])
    header = nib.Nifti2Header()
    header.set_data_shape(shape)
    header.set_data_dtype(omz["0"].dtype)
    header.set_qform(affine)
    header.set_sform(affine)
    header.set_xyzt_units(nib.nifti1.unit_codes.code["micron"])
    header.structarr["magic"] = b"nz2\0"
    header = np.frombuffer(header.structarr.tobytes(), dtype="u1")
    opt = {
        "chunks": [len(header)],
        "dimension_separator": r"/",
        "order": "F",
        "dtype": "|u1",
        "fill_value": None,
        "compressor": None,
    }
    omz.create_dataset("nifti", data=header, shape=shape, **opt)
    print("done.")
