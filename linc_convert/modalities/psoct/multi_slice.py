"""
Matlab to OME-Zarr.

Converts Matlab files generated by the MGH in-house OCT pipeline
into a OME-ZARR pyramid.
"""

import ast
import json
import math
import os
from functools import wraps
from itertools import product
from typing import Callable, Mapping, Optional
from warnings import warn

import cyclopts
import h5py
import numpy as np
import zarr
from scipy.io import loadmat

from linc_convert.modalities.psoct._utils import (
    generate_pyramid,
    make_json,
    niftizarr_write_header,
    write_ome_metadata,
)
from linc_convert.modalities.psoct.cli import psoct
from linc_convert.utils.math import ceildiv
from linc_convert.utils.orientation import center_affine, orientation_to_affine
from linc_convert.utils.unit import to_nifti_unit, to_ome_unit
from linc_convert.utils.zarr import make_compressor

multi_slice = cyclopts.App(name="multi_slice", help_format="markdown")
psoct.command(multi_slice)


def _automap(func: Callable) -> Callable:
    """Automatically maps the array in the mat file."""

    @wraps(func)
    def wrapper(inp: list[str], out: str = None, **kwargs: dict) -> callable:
        if out is None:
            out = os.path.splitext(inp[0])[0]
            out += ".nii.zarr" if kwargs.get("nii", False) else ".ome.zarr"
        kwargs["nii"] = kwargs.get("nii", False) or out.endswith(".nii.zarr")
        dat = _mapmat(inp, kwargs.get("key", None))
        return func(dat, out, **kwargs)

    return wrapper


class _ArrayWrapper:
    def _get_key(self, f: Mapping) -> str:
        key = self.key
        if key is None:
            if not len(f.keys()):
                raise Exception(f"{self.file} is empty")
            for key in f.keys():
                if key[:1] != "_":
                    break
            if len(f.keys()) > 1:
                warn(
                    f"More than one key in .mat file {self.file}, "
                    f'arbitrarily loading "{key}"'
                )

        if key not in f.keys():
            raise Exception(f"Key {key} not found in file {self.file}")

        return key


class _H5ArrayWrapper(_ArrayWrapper):
    def __init__(self, file: h5py.File, key: str | None) -> None:
        self.file = file
        self.key = key
        self.array = file.get(self._get_key(self.file))

    def __del__(self) -> None:
        if hasattr(self.file, "close"):
            self.file.close()

    def load(self) -> np.ndarray:
        self.array = self.array[...]
        if hasattr(self.file, "close"):
            self.file.close()
        self.file = None
        return self.array

    @property
    def shape(self) -> list[int]:
        return self.array.shape

    @property
    def dtype(self) -> np.dtype:
        return self.array.dtype

    def __len__(self) -> int:
        return len(self.array)

    def __getitem__(self, index: object) -> np.ndarray:
        return self.array[index]


class _MatArrayWrapper(_ArrayWrapper):
    def __init__(self, file: str, key: str | None) -> None:
        self.file = file
        self.key = key
        self.array = None

    def __del__(self) -> None:
        if hasattr(self.file, "close"):
            self.file.close()

    def load(self) -> np.ndarray:
        f = loadmat(self.file)
        self.array = f.get(self._get_key(f))
        self.file = None
        return self.array

    @property
    def shape(self) -> list[int]:
        if self.array is None:
            self.load()
        return self.array.shape

    @property
    def dtype(self) -> np.dtype:
        if self.array is None:
            self.load()
        return self.array.dtype

    def __len__(self) -> int:
        if self.array is None:
            self.load()
        return len(self.array)

    def __getitem__(self, index: object) -> np.ndarray:
        if self.array is None:
            self.load()
        return self.array[index]


def _mapmat(fnames: list[str], key: str = None) -> list[_ArrayWrapper]:
    """Load or memory-map an array stored in a .mat file."""
    # loaded_data = []

    def make_wrapper(fname: str) -> callable:
        try:
            # "New" .mat file
            f = h5py.File(fname, "r")
            return _H5ArrayWrapper(f, key)
        except Exception:
            # "Old" .mat file
            return _MatArrayWrapper(fname, key)

    return [make_wrapper(fname) for fname in fnames]


@multi_slice.default
@_automap
def convert(
    inp: list[str],
    out: Optional[str] = None,
    *,
    key: Optional[str] = None,
    meta: str = None,
    chunk: int = 128,
    compressor: str = "blosc",
    compressor_opt: str = "{}",
    max_load: int = 128,
    max_levels: int = 5,
    no_pool: Optional[int] = None,
    nii: bool = False,
    orientation: str = "RAS",
    center: bool = True,
    dtype: str | None = None,
) -> None:
    """
    Matlab to OME-Zarr.

    Convert OCT volumes in raw matlab files into a pyramidal
    OME-ZARR (or NIfTI-Zarr) hierarchy.

    This command assumes that each slice in a volume is stored in a
    different mat file. All slices must have the same shape, and will
    be concatenated into a 3D Zarr.

    Parameters
    ----------
    inp
        Path to the input mat file
    out
        Path to the output Zarr directory [<INP>.ome.zarr]
    key
        Key of the array to be extracted, default to first key found
    meta
        Path to the metadata file
    chunk
        Output chunk size
    compressor : {blosc, zlib, raw}
        Compression method
    compressor_opt
        Compression options
    max_load
        Maximum input chunk size
    max_levels
        Maximum number of pyramid levels
    no_pool
        Index of dimension to not pool when building pyramid.
    nii
        Convert to nifti-zarr. True if path ends in ".nii.zarr"
    orientation
        Orientation of the volume
    center
        Set RAS[0, 0, 0] at FOV center
    dtype
        Data type to write into
    """
    if isinstance(compressor_opt, str):
        compressor_opt = ast.literal_eval(compressor_opt)

    # Write OME-Zarr multiscale metadata
    if meta:
        print("Write JSON")
        with open(meta, "r") as f:
            meta_txt = f.read()
            meta_json = make_json(meta_txt)
        path_json = ".".join(out.split(".")[:-2]) + ".json"
        with open(path_json, "w") as f:
            json.dump(meta_json, f, indent=4)
        vx = meta_json["PixelSize"]
        unit = meta_json["PixelSizeUnits"]
    else:
        vx = [1] * 3
        unit = "um"

    # Prepare Zarr group
    omz = zarr.storage.DirectoryStore(out)
    omz = zarr.group(store=omz, overwrite=True)

    # if not hasattr(inp[0], "dtype"):
    #     raise Exception("Input is not an array. This is likely unexpected")
    if len(inp[0].shape) < 2:
        raise Exception("Input array is not 2d:", inp[0].shape)
    # Prepare chunking options
    dtype = dtype or np.dtype(inp[0].dtype).str
    opt = {
        "dimension_separator": r"/",
        "order": "F",
        "dtype": dtype,
        "fill_value": None,
        "compressor": make_compressor(compressor, **compressor_opt),
    }
    inp: list = inp
    inp_shape = (*inp[0].shape, len(inp))
    inp_chunk = [min(x, max_load) for x in inp_shape[-3:]]
    nk = ceildiv(inp_shape[-3], inp_chunk[0])
    nj = ceildiv(inp_shape[-2], inp_chunk[1])
    ni = len(inp)

    nblevels = min(
        [
            int(math.ceil(math.log2(x)))
            for i, x in enumerate(inp_shape[-3:])
            if i != no_pool
        ]
    )
    nblevels = min(nblevels, int(math.ceil(math.log2(max_load))))
    nblevels = min(nblevels, max_levels)

    opt["chunks"] = [min(x, chunk) for x in inp_shape]

    omz.create_dataset(str(0), shape=inp_shape, **opt)

    # iterate across input chunks
    for i in range(ni):
        for j, k in product(range(nj), range(nk)):
            loaded_chunk = inp[i][
                ...,
                k * inp_chunk[0] : (k + 1) * inp_chunk[0],
                j * inp_chunk[1] : (j + 1) * inp_chunk[1],
            ]

            print(
                f"[{i + 1:03d}, {j + 1:03d}, {k + 1:03d}]",
                "/",
                f"[{ni:03d}, {nj:03d}, {nk:03d}]",
                # f"({1 + level}/{nblevels})",
                end="\r",
            )

            # save current chunk
            omz["0"][
                ...,
                k * inp_chunk[-3] : k * inp_chunk[-3] + loaded_chunk.shape[-2],
                j * inp_chunk[-2] : j * inp_chunk[-2] + loaded_chunk.shape[-1],
                i,
            ] = loaded_chunk

        inp[i] = None  # no ref count -> delete array

    generate_pyramid(omz, nblevels - 1, mode="mean", no_pyramid_axis=no_pool)

    print("")

    # Write OME-Zarr multiscale metadata
    print("Write metadata")
    print(unit)
    ome_unit = to_ome_unit(unit)
    write_ome_metadata(
        omz,
        axes=(["c"] if len(inp_shape) == 4 else []) + ["z", "y", "x"],
        no_pool=no_pool,
        space_unit=ome_unit,
        space_scale=vx,
        multiscales_type=(("2x2x2" if no_pool is None else "2x2") + "mean window"),
    )

    if not nii:
        print("done.")
        return

    # Write NIfTI-Zarr header
    # NOTE: we use nifti2 because dimensions typically do not fit in a short
    # TODO: we do not write the json zattrs, but it should be added in
    #       once the nifti-zarr package is released
    shape = list(reversed(omz["0"].shape))
    affine = orientation_to_affine(orientation, *vx[::-1])
    if center:
        affine = center_affine(affine, shape[:3])
    niftizarr_write_header(
        omz, shape, affine, omz["0"].dtype, to_nifti_unit(unit), nifti_version=2
    )
